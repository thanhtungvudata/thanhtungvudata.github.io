---
title: "A/B Testing with Highly-Skewed Data Using the Mann–Whitney U Test"
date: 2025-01-07
categories:
  - Data Science Fundamentals
tags:
  - Data Science
---

When your data is highly skewed, traditional A/B tests like the t-test or z-test (see my previous posts [P1](https://thanhtungvudata.github.io/data%20science%20fundamentals/AB-testing/) and [P2](https://thanhtungvudata.github.io/data%20science%20fundamentals/AB-testing2/)) can give misleading results. These tests assume that data is normally distributed, or that the sample size is large enough for the Central Limit Theorem to smooth out the distribution.

But what if you're comparing user spend, session times, or click counts, where a few large values dominate? In these cases, using the Mann–Whitney U (MWU) test can provide more robust and reliable results.

<img src="/assets/images/AB_testing_MWU_test.png" alt="AB Testing MWU-Test" width="600">

### 📌 What Is the Mann–Whitney U Test?

The MWU test (also known as the Wilcoxon rank-sum test) is a non-parametric test used to compare two independent groups. It does not assume normality and works by comparing the ranks of values instead of the values themselves.

✅ Key Features:

- Compares distributions, not means
- Robust to outliers and skewed data
- Tests whether one group tends to have larger values than the other

### Workflow for A/B testing with MWU-Test

1. Formulate Your Hypothesis
- Example: "Users who see Version B spend more time on the page than those who see Version A."
2. Check for Skew
- Plot histograms or use scipy.stats.skew().
- If data is heavily skewed, avoid the t-test.
3. Use the Mann–Whitney U Test
- Convert values into ranks.
- Compare the rank distributions of Group A and Group B.
4. Interpret the p-value
- A low p-value (e.g. < 0.05) suggests a statistically significant difference between groups.

### Theory for A/B Testing with MWU-Test

#### 🎯 Problem Setup

Suppose you're testing a new landing page, and your metric of interest is the session duration between two versions of a landing page. You have:

- Group A (control): current landing page
- Group B (variant): new landing page

Suppose: 
- $$X_1, X_2, \dots, X_{n_A}$$ are the observed session durations in A
- $$Y_1, Y_2, \dots, Y_{n_B}$$ are the observed session durations in B


🔍 Step 1: Define Hypotheses

The Mann–Whitney U test tests:

- Null hypothesis $$H_0$$: there is no tendency for one group's session durations to be systematically higher or lower than the other (i.e., they come from the same distribution), which is

$$H_0: P(X < Y) + 0.5 P(X = Y) =  0.5, \,\, \text{(two-sided test)}$$

$$P(Y < X) + 0.5 P(Y = X) = 0.5$$

$$P(X < Y) = P(Y < X)$$

or Group B tends to have larger values than Group A, i.e., 

$$H_0: P(X < Y) \leq 0.5, \,\, \text{(one-sided test)}$$ 

- Alternative hypothesis $$H_1$$: Variant B performs differently, i.e.,

$$H_1: P(X < Y) \neq P(Y < X) \,\, \text{(two-sided test)}$$

$$P(X < Y) + 0.5 P(X = Y) \neq 0.5$$

$$P(Y < X) + 0.5 P(Y = X) \neq 0.5$$

or if B does not perform better, i.e.,

$$H_1: P(X < Y) > 0.5, \,\, \text{(one-sided test)}$$ 



⚙️ Step 2: MWU Statistic Under $$H_0$$

The Mann–Whitney U statistic counts the number of times an observation from one group is less than or equal an observation from the other group:

$$U_A = \sum_{i=1}^{n_A} \sum_{j=1}^{n_B} \mathbb{I}_{(X_i < Y_j)}$$ 

$$U_B = \sum_{i=1}^{n_A} \sum_{j=1}^{n_B} \mathbb{I}_{(Y_j < X_i)}$$ 

where $$\mathbb{I}_{a} = 1$$ if $$a$$ is true, and $$\mathbb{I}_{a} = 0$$, otherwise. 

Computing $$U_A$$ or $$U_B$$ using the above definition requires $$n_A n_B$$ comparison, which has time complexity of $$\mathcal{O}(n_A n_B)$$ and becomes very expensive if the sample sizes are large. Therefore, we will use a different way that is more computationally efficient:

$$U_A = n_An_B + \frac{n_A (n_A+1)}{2} - R_A$$

$$U_B = n_An_B + \frac{n_B (n_B+1)}{2} - R_B$$

where $$R_A = \sum_{i=1}^{n_A} \text{rank}(X_i)$$, $$R_B = \sum_{j=1}^{n_B} \text{rank}(Y_j)$$ and $$\text{rank}(X_i)$$ or $$\text{rank}(Y_j)$$ is the rank of $$X_i$$ or $$Y_j$$ in the joint list of both groups A and B. 

The explanation of the formula computing the $$U_A$$ or $$U_B$$ using ranks will be discussed at the end of this post. 

🧮 Step 3: T-Statistic Under $$H_0$$

The z-score tells us how far the difference of the observed average time on page we see is from zero, using standard error as the unit, assuming that $$H_0$$ is true:

$$t = \frac{\bar{\mu}_B - \bar{\mu}_A}{\text{SE}}$$

So, 
- If $$t$$ is close to 0, the observed difference is what we'd expect from random chance
- If $$t$$ is far from 0, the difference is larger than what we'd expect from chance, so it might be statistically significant

📉 Step 4: Compute P-Value Under $$H_0$$

Just knowing "how many standard errors away" from the computed z-score is isn't enough — we want to **quantify how likely** it is to observe such a result by chance. Therefore, we need to compute "p-value", which is the probability of obtaining a result as extreme as (or more extreme than) your observed data, assuming that the null hypothesis $$H_0$$ is true.

To do this, given the t-score formula above, we use the t-distribution (instead of the normal distribution) to model this probability because we have extra uncertainty due to estimating standard deviations from small samples. 

The **assumption** for using the t-distribution is that **the data is approximately normal** so that standard deviations from small samples (e.g., $$s_A$$) is a **good estmate** of the true standard deviation (e.g., $$\sigma_A$$).

Unlike the normal distribution (which has a fixed shape), the t-distribution changes shape depending on the degrees of freedom (DOF). The t-distribution accounts for extra uncertainty that arises when we estimate the population standard deviation from a small sample. That uncertainty depends on the sample size — and that's where degrees of freedom (df) come in.

- Smaller sample size → smaller DOF → more uncertainty → heavier tails.
- Larger sample size → larger DOF → less uncertainty → closer to standard normal distribution.

Therefore, choosing the correct DOF is critical for an accurate p-value.

This is the Welch–Satterthwaite formula for DOF, which is commonly used in practice:

$$\text{DOF} = \frac{ \Big( \frac{s_A^2}{n_A} + \frac{s_B^2}{n_B} \Big)^2 }{ \frac{ \Big( \frac{s_A^2}{n_A} \Big)^2 }{n_A - 1} + \frac{ \Big( \frac{s_B^2}{n_B} \Big)^2 }{n_B - 1} }$$

(This DOF formula will be explained in more detail at the end of this post)

We use the t-score to find a p-value from the t-distribution. 

The p-value answers this question: "If the null hypothesis $$H_0$$ is true, what is the probability of seeing a result this extreme or more extreme just by chance?"

Mathematically, p-value is the area under the curve of the t-distribution beyond your t-score.

- For a two-tailed test:

$$p_{value} = 2 P (T > |t|)$$

- For a one-tailed test:

$$p_{value} = P (T > |t|)$$

where $$P (.)$$ is the probability that is computed using a t-distribution with the DOF we calculated earlier.

✅ Step 5: Decide to reject $$H_0$$ or not

- If $$p_{value} < \alpha$$ (commonly $$0.05$$), we reject $$H_0$$ and conclude that the difference is statistically significant.
- Otherwise, we fail to reject $$H_0$$.

### Implementing an Example A/B Test in Python
Let’s simulate a basic A/B test for the average time on page.

```python
from scipy.stats import ttest_ind

# Simulated time-on-page data (in seconds)
group_A = [120, 130, 115, 123, 140]
group_B = [150, 160, 145, 155, 170]

# Welch's t-test (does not assume equal variances)
t_stat, p_value = ttest_ind(group_B, group_A, equal_var=False)

print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_value:.4f}")
```

Output:

```bash
t-statistic: 4.9736
p-value: 0.0011
```

With a p-value of 0.0011 significantly smaller than 0.05, we reject the null hypothesis and conclude that Group B performs significantly better.

### 🧪 When and Why to Use the t-Test in A/B Testing

✅ Use the t-test when:

- **Comparing Means**: Use the t-test when your goal is to compare the average performance of two groups (e.g., average time on page or purchase amount).
- **Unknown Population Variance**: The population standard deviation is usually unknown, so you need to estimate it from your sample.
- **Small to Moderate Sample Sizes**: While a common rule of thumb is to use the t-test when the sample size is less than 30 per group, it remains useful even with slightly larger samples when the population variance is unknown.
- **Approximately Normal Data**: The t-test assumes that the underlying data is approximately normally distributed. This assumption is particularly important when the sample size is very small.

❌ Don’t use the t-test when:

- **Comparing Proportions Directly**: While a t-test can technically be used in some transformed or regression-based comparisons of proportions, in standard A/B testing practice, we typically use a z-test for comparing two proportions (like conversion rates), because proportions follow a binomial distribution. Proportions are not continuous variables like those in t-test, and their variance depends on the proportion itself. For very small samples, Fisher's exact test is more appropriate.
- **Severely Non-Normal Data in Very Small Samples**: When the data is heavily skewed or not normally distributed, and the sample size is extremely small, the t-test may not be reliable due to biased or unstable standard deviation estimate of the data samples. In such cases, consider non-parametric alternatives like the Mann-Whitney U test.

💡 Real-World Advice:

The t-test is usually safe and practical in A/B testing, product analytics, and experiments as long as your sample isn't too tiny or your data isn't wildly non-normal.

## Summary
The t-test is a powerful tool for A/B testing when your sample size is small and you’re comparing means. It accounts for the added uncertainty in small datasets, allowing you to make data-informed decisions — even when data is limited.

---
🚀 The code of the example is available [here](https://github.com/thanhtungvudata/Data_Science_Fundamentals). 

For further inquiries or collaboration, please contact me at [my email](mailto:tungvutelecom@gmail.com).

---
### Explanation for the Formula of Computing $$U_A$$ or $$U_B$$ using Ranks:

If all $$X_i$$ values were smaller than all $$Y_j$$ values, all these $$X_i$$ would receive the lowest ranks: $$1,2,\dots,n_A$$. Therefore,

$$R_{A,\min} = \sum_{i=1}^{n_A} \text{min rank of} X_i = \frac{n_A(n_A+1)}{2}$$

Every time an $$X_i$$ larger than $$Y_j$$, the rank of $$X_i$$ is pushed higher compared to its lowest ranks.