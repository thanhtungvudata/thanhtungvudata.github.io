---
title: "A/B Testing with Highly-Skewed Data Using the Mannâ€“Whitney U Test"
date: 2025-01-07
categories:
  - Data Science Fundamentals
tags:
  - Data Science
---

When your data is highly skewed, traditional A/B tests like the t-test or z-test (see my previous posts [P1](https://thanhtungvudata.github.io/data%20science%20fundamentals/AB-testing/) and [P2](https://thanhtungvudata.github.io/data%20science%20fundamentals/AB-testing2/)) can give misleading results. These tests assume that data is normally distributed, or that the sample size is large enough for the Central Limit Theorem to smooth out the distribution.

But what if you're comparing user spend, session times, or click counts, where a few large values dominate? In these cases, using the Mannâ€“Whitney U (MWU) test can provide more robust and reliable results.

<img src="/assets/images/AB_testing_MWU_test.png" alt="AB Testing MWU-Test" width="600">

### ğŸ“Œ What Is the Mannâ€“Whitney U Test?

The MWU test (also known as the Wilcoxon rank-sum test) is a non-parametric test used to compare two independent groups. It does not assume normality and works by comparing the ranks of values instead of the values themselves.

âœ… Key Features:

- Compares distributions, not means
- Robust to outliers and skewed data
- Tests whether one group tends to have larger values than the other

### Workflow for A/B testing with MWU-Test

1. Formulate Your Hypothesis
- Example: "Users who see Version B spend more time on the page than those who see Version A."
2. Check for Skew
- Plot histograms or use scipy.stats.skew().
- If data is heavily skewed, avoid the t-test.
3. Use the Mannâ€“Whitney U Test
- Convert values into ranks.
- Calculate the U Statistic from the ranks.
- Compute the z-score from the U Statistic
4. Interpret the p-value
- Compute p-value from the computed z-score.
- A low p-value (e.g. < 0.05) suggests a statistically significant difference between groups.

### Theory for A/B Testing with MWU-Test

#### ğŸ¯ Problem Setup

Suppose you're testing a new landing page, and your metric of interest is the session duration between two versions of a landing page. You have:

- Group A (control): current landing page
- Group B (variant): new landing page

Suppose: 
- $$X_1, X_2, \dots, X_{n_A}$$ are the observed session durations in A
- $$Y_1, Y_2, \dots, Y_{n_B}$$ are the observed session durations in B


ğŸ” Step 1: Define Hypotheses

The Mannâ€“Whitney U test tests:

- Null hypothesis $$H_0$$: there is no tendency for one group's session durations to be systematically higher or lower than the other (i.e., they come from the same distribution), which is

$$H_0: P(X < Y) + 0.5 P(X = Y) =  0.5, \,\, \text{(two-sided test)}$$

$$P(Y < X) + 0.5 P(Y = X) = 0.5$$

$$P(X < Y) = P(Y < X)$$

or Group B tends to have larger values than Group A, i.e., 

$$H_0: P(X < Y) \leq 0.5, \,\, \text{(one-sided test)}$$ 

- Alternative hypothesis $$H_1$$: Variant B performs differently, i.e.,

$$H_1: P(X < Y) \neq P(Y < X) \,\, \text{(two-sided test)}$$

$$P(X < Y) + 0.5 P(X = Y) \neq 0.5$$

$$P(Y < X) + 0.5 P(Y = X) \neq 0.5$$

or ifÂ B does not perform better, i.e.,

$$H_1: P(X < Y) > 0.5, \,\, \text{(one-sided test)}$$ 


âš™ï¸ Step 2: MWU Statistic Under $$H_0$$

The Mannâ€“Whitney U statistic counts the number of times an observation from one group is less than or equal an observation from the other group:

$$U_A = \sum_{i=1}^{n_A} \sum_{j=1}^{n_B} \mathbb{I}_{(X_i < Y_j)}$$ 

$$U_B = \sum_{i=1}^{n_A} \sum_{j=1}^{n_B} \mathbb{I}_{(Y_j < X_i)}$$ 

where $$\mathbb{I}_{a} = 1$$ if $$a$$ is true, and $$\mathbb{I}_{a} = 0$$, otherwise. 

Computing $$U_A$$ or $$U_B$$ using the above definition requires $$n_A n_B$$ comparison, which has time complexity of $$\mathcal{O}(n_A n_B)$$ and becomes very expensive if the sample sizes are large. 

Therefore, we will use a different way that is more computationally efficient using ranks of elements in both groups:

$$U_A = n_An_B + \frac{n_A (n_A+1)}{2} - R_A$$

$$U_B = n_An_B + \frac{n_B (n_B+1)}{2} - R_B$$

where $$R_A = \sum_{i=1}^{n_A} \text{rank}(X_i)$$, $$R_B = \sum_{j=1}^{n_B} \text{rank}(Y_j)$$ and $$\text{rank}(X_i)$$ or $$\text{rank}(Y_j)$$ is the rank of $$X_i$$ or $$Y_j$$ in the joint list of both groups A and B. 

The explanation of the formula computing the $$U_A$$ or $$U_B$$ using ranks will be discussed at the end of this post. 

Here, the step of "converting values into ranks" in the workflow can also reduce the influence of extreme values and enables the test to focus on the order of the data rather than the specific values.


ğŸ§® Step 3: U-Statistic Under $$H_0$$

For large samples (typically $$n_A, n_B \geq 20$$), the central limit theorem (CLT) tells us that the U statistic is approximately normally distributed because $$U_A$$ or $$U_B$$ is the sum of $$n_An_B$$ random variables. 

The mean of $$U_A$$ or $$U_B$$ under $$H_0$$ is

$$\mu_U = \frac{n_An_B}{2}$$

The standard deviation of $$U_A$$ or $$U_B$$ under $$H_0$$ is

$$\sigma_U = \sqrt{\frac{n_An_B}{12}(N+1 - T)}$$

where $$N = n_A+n_B$$ and

$$T = \frac{\sum_{i=1}^g (t_i^3-t_i)}{N(N-1)}$$

(the explanation of these statistic parameters will be discussed at the end of this post)

The z-score tells us how far the difference of the observed session duration we see is from zero, using standard error as the unit, assuming that $$H_0$$ is true:

$$z = \frac{U-\mu_U}{\sigma_U}$$

In practice, we often use $$U = \min(U_A,U_B)$$ by convention to avoid confusion in one-sided tests.

However, mathematically, either $$U_A$$ or $$U_B$$ can be used as $$U$$ and give the same z-score because they share the same mean and variance. 

So, 
- If $$z$$ is close to 0, the observed difference is what we'd expect from random chance
- If $$z$$ is far from 0, the difference is larger than what we'd expect from chance, so it might be statistically significant

ğŸ“‰ Step 4: Compute P-Value Under $$H_0$$

If the sample sizes are large, the steps of computing p-values based on the compuated z-score are similar to those in my previous [post](https://thanhtungvudata.github.io/data%20science%20fundamentals/AB-testing/), and hence, omitted here. 

If the sample sizes are small, we can calculate the p-value by using the exact distribution of $$U$$, which takes into account the discrete nature of the test statistic rather than approximating it with a continuous normal distribution. We will not discuss this case in this post since the post is already too long. 

### Implementing an Example A/B Test in Python
Letâ€™s simulate a basic A/B test for the average time on page.

```python
from scipy.stats import ttest_ind

# Simulated time-on-page data (in seconds)
group_A = [120, 130, 115, 123, 140]
group_B = [150, 160, 145, 155, 170]

# Welch's t-test (does not assume equal variances)
t_stat, p_value = ttest_ind(group_B, group_A, equal_var=False)

print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_value:.4f}")
```

Output:

```bash
t-statistic: 4.9736
p-value: 0.0011
```

With a p-value of 0.0011 significantly smaller than 0.05, we reject the null hypothesis and conclude that Group B performs significantly better.


## Summary
The t-test is a powerful tool for A/B testing when your sample size is small and youâ€™re comparing means. It accounts for the added uncertainty in small datasets, allowing you to make data-informed decisions â€” even when data is limited.

---
ğŸš€ The code of the example is available [here](https://github.com/thanhtungvudata/Data_Science_Fundamentals). 

For further inquiries or collaboration, please contact me at [my email](mailto:tungvutelecom@gmail.com).

---
### Explanation for the Formula of Computing $$U_A$$ or $$U_B$$ using Ranks:

If all $$X_i$$ values were smaller than all $$Y_j$$ values, all these $$X_i$$ would receive the lowest ranks: $$1,2,\dots,n_A$$. Therefore,

$$R_{A,\min} = \sum_{i=1}^{n_A} \text{min rank of} X_i = \frac{n_A(n_A+1)}{2}$$

Every time an $$X_i$$ larger than $$Y_j$$, the rank of $$X_i$$ is pushed higher compared to its lowest ranks.