---
title: "A Guide to LLM Pre-training: Teaching Machines to Think"
date: 2025-01-08
categories:
  - LLM Pre-training
tags:
  - Data Science
  - Generative AI
  - LLM
---

Large Language Models (LLMs) like GPT-4 and ChatGPT are powerful tools that understand and generate human-like text. But how do they get so smart in the first place? The answer lies in **pre-training** ‚Äî a foundational step in building these intelligent systems.

In this post, we‚Äôll break down:

- ‚úÖ What LLM pre-training does
- üí° Why it‚Äôs necessary
- ‚öôÔ∏è How it works under the hood

---

## ‚úÖ What Is Pre-training?

**Pre-training** is the process of teaching a large language model general language skills by exposing it to massive amounts of text data. The model learns to **predict the next token** (word or subword) given previous tokens, a task known as **causal language modeling**.

For example, given the sentence:

> "The capital of France is \_\_\_"

The model learns to predict the most likely next word ‚Äî "Paris" ‚Äî based on the context.

This single objective turns out to be incredibly powerful: by learning to predict the next token, the model acquires knowledge about syntax, semantics, world facts, reasoning, and more.

---

## üí° Why Is Pre-training Needed?

1. **Foundation of Knowledge**

   - Pre-training equips the model with a broad understanding of language, facts, and logic, much like how humans learn from reading.

2. **Scalability**

   - It uses **self-supervised learning**, meaning it doesn‚Äôt require manually labeled data.

3. **Transferability**

   - Once pre-trained, the model can be fine-tuned or adapted to many downstream tasks: summarization, coding, translation, and more.

4. **Emergent Capabilities**

   - Larger pre-trained models start to show surprising skills like arithmetic, chain-of-thought reasoning, and even code generation, without being explicitly trained for them.

---

## ‚öôÔ∏è How LLM Pre-training Works

### 1. **Input Processing**

- The raw input text is first tokenized into a sequence of discrete tokens: $$ x_1, x_2, \dots, x_T $$.
- Each token $$ x_t $$ is mapped to a **token embedding vector** using a learnable embedding matrix $$ W^E \in \mathbb{R}^{V \times d} $$, where $$ V $$ is the vocabulary size and $$ d $$ is the model's hidden dimension.
- $$ W^E[x_t] $$ denotes the **lookup operation**, retrieving the row vector in $$ W^E $$ corresponding to the token ID $$ x_t $$. This produces an embedding vector of dimension $$ \mathbb{R}^d $$.
- Simultaneously, a **positional embedding vector** is added to each token to encode its position in the sequence. This is provided by another learnable matrix $$ W^P \in \mathbb{R}^{T \times d} $$, where $$ T $$ is the maximum sequence length.

- The result is a sequence of input vectors:

  $$
  h_0^{(t)} = W^E[x_t] + W^P[t] \in \mathbb{R}^d
  $$

  for each position $$ t = 1, 2, \dots, T $$. This sequence $$ h_0^{(1)}, h_0^{(2)}, \dots, h_0^{(T)} $$ forms the initial input to the first Transformer layer.

- Both $$ W^E $$ and $$ W^P $$ are **learnable parameters** and are part of the model‚Äôs overall parameter set $$ \theta $$. These are updated during training via backpropagation to improve the model's language understanding capabilities.

- These enriched input representations are then passed through the stacked Transformer layers, where more complex contextual features are learned at deeper levels of the network.

### 2. **Transformer Layers**

- These layers take the embedded input sequence and transform it through a stack of self-attention and feedforward blocks, producing a **contextualized representation** for each token in the sequence.

- The output for each token position $$t$$ after the final Transformer layer is a vector $$h_t$$, which captures its meaning in context.

- Each Transformer layer consists of two main sub-blocks:

  1. **Self-attention block**
  2. **Feedforward block**

- Both sub-blocks are wrapped with:

  - **Residual connection** (Add): the input is added to the output of the sub-block.
  - **Layer Normalization** (LayerNorm): applied either before or after the sub-block (depending on architecture).

- Each LayerNorm includes **learnable parameters**:

  - **Gain**: $$\gamma \in \mathbb{R}^{d}$$
  - **Bias**: $$\beta \in \mathbb{R}^{d}$$

- Summary of learnable components per layer:

  - **Self-attention**: $$W^Q, W^K, W^V, W^O$$
  - **Feedforward Network (FFN)**: $$W_1, W_2, b_1, b_2$$
  - **LayerNorm Parameters**: $$\gamma, \beta$$ for each normalization layer



### 3. **Output Projection**

- The final hidden state $$h_t$$ is passed to a **language modeling head**:

  $$
  \text{logits}_t = h_t \cdot W^{LM}
  $$

  - $$W^{LM} \in \mathbb{R}^{d \times V}$$ projects the hidden state to a vector of vocabulary-sized logits. Each element in the resulting vector corresponds to a score (logit) for one vocabulary token.
  - A softmax turns logits into probabilities over all possible next tokens:

  $$
  P_\theta(x_{t+1} = x^* \mid x_{\leq t}) = \frac{\exp(\text{logits}_t[x^*])}{\sum_{j=1}^{V} \exp(\text{logits}_t[j])}
  $$

  where:
  - $$x^*$$ is the actual next token,
  - $$\text{logits}_t[x^*]$$ is its unnormalized score,
  - and $$V$$ is the vocabulary size.



### 4. **Loss Function**

- The model is trained to minimize the **cross-entropy loss** between the predicted distribution and the actual next token:

  $$\mathcal{L}(\theta) = -\sum_{t=1}^{T} \log P_\theta(x_t \mid x_{<t})$$

- This is optimized using **stochastic gradient descent (SGD)** or adaptive variants like AdamW.

---

## üéØ Final Thoughts

Pre-training is where LLMs gain their general intelligence. It's an expensive, compute-heavy process ‚Äî but it's what makes ChatGPT and similar tools so versatile, fluent, and helpful.

Next time you ask an LLM a question and get a coherent response, remember: it‚Äôs thanks to billions of tokens, trillions of computations, and the power of pre-training.

---

üß† **Up Next:** Want to dive into fine-tuning and how LLMs adapt to specific tasks? Stay tuned for the next post!

For further inquiries or collaboration, feel free to contact me at [my email](mailto\:tungvutelecom@gmail.com).








